---
title: "Confidence Intervals"
author: "Math 271"
date: "3/22/2022"
output: 
  html_document:
    css: lab.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library(tidyverse)
library(magrittr)
library(moderndive)
library(purrr)
library(ggplot2)

```

## Bootstrapping setup

```{r bootstrap}

bind_group <- function(data, .id="run"){
  data %>% bind_rows(.id=.id) %>% group_by(.data[[.id]], .add=TRUE)
}

bootstrap_means <- rerun(1000, pennies_sample %>% slice_sample(prop=1, replace=TRUE)) %>% 
  bind_group(.id="sample") %>% 
  summarize(mean_year= mean(year))

(mean_hist <- ggplot(bootstrap_means) + 
    aes(x=mean_year) + 
    geom_histogram(center=0, binwidth=1) + 
  xlim(1985, 2005))
```
 

### Three types of CI: two-sided interval, lower-bound, upper-bound

Two-sided interval: Statement: _I'm confident that the population mean is somewhere in this range of numbers._

one-sided intervals: bounds _I'm confident that the population mean is less than some number_. (upper-bound)


probability idea: percentiles (quantiles)  0%-100% vs 0.0 - 1.0

Percentile e.g. if someone is found in the 90th percentile, they are in the top 10%.

A method to represent the upper bound:

```{r}
level = 0.99 

quantile(bootstrap_means$mean_year, level)
```

```{r}
mean_hist + geom_vline(xintercept = quantile(bootstrap_means$mean_year, level), col="red")
```


```{r}
## this was a side discussion about using max for confidence interval -----------
quantile(bootstrap_means$mean_year, 1.0)
max(bootstrap_means$mean_year)
## ----------
```

When building confidence intervals, we are responsible for developing confidence levels, in the example above the confidence level is set to .99. This means that 99% of the data falls below this point, and 1% of the data falls above this point.

NOTE: The more we increase the upper bound of our confidence interval, the less precise our statements as we put an increasing amount of data on the left side of the upper bound. 

For example:
I'm 99% "confident" that the population mean of penny minting years is less than 2000.2.

"confident" = If I use this method to create an upper bound, then at least 99% of samples will produce a confidence interval that is "correct". 

"correct" = The statement about the real (unknown) population mean is true. 


#### What about a _lower-bound_?

```{r}
level = 0.90 

quantile(bootstrap_means$mean_year, 1 - level)
mean_hist + geom_vline(xintercept = quantile(bootstrap_means$mean_year, 1 - level), col="red")
```

Note: A lower bound confidence level of .90 means 90% of data on right and 10% on the left. A way to do this is by taking the compliment (1 - upper bound) of the upper bound interval of .90.


### How to compute two-sided probabilities `(1-level)/2` and `(1+level)/2`

Now: (e.g.) 90% confidence means 90% in the middle, and 10% on the "outside". (split it up evenly 5% on each side)

90% confidence level -> 10% outside -> (even split) 5% on bottom and 5% on the top

quantiles to lookup: 0.05 (on bottom) and 0.95 (0.05 on top)

We can do this by making our "level" a vector of 0.05 an 0.95:
```{r}
level = c(0.05, 0.95) 

quantile(bootstrap_means$mean_year, level)
mean_hist + geom_vline(xintercept = quantile(bootstrap_means$mean_year, level), col="red")
```


### Digression: [Anonymous] Functions, pipelines 

Here we are creating a function that will return the quantiles to lookup when producing a two-sided confidence interval for a given `level` (e.g. given .90, it will produce .05 and .95):
```{r}

#fully explicit efficient way:
ci_probs <- function(level){
lower = (1-level) / 2
upper = (1+level) / 2
c(lower, upper)
}

# a small algebraic modification
ci_probs <- function(level) {
  c(1 + (-1) * level, 
    1 + (+1) * level) / 2
}

# a further simplification of the last way
ci_probs <- function(level){
  (1 + c(-1,1) * level) / 2
}

# alternative implementation using pipes
ci_probs <- function(level){
  level %>% multiply_by(c(-1,1)) %>% add(1) %>% divide_by(2)
}

# example showing how using a period (.) with a pipeline creates a function:
ci_probs <- . %>% multiply_by(c(-1,1)) %>% add(1) %>% divide_by(2)


ci_probs(0.90)

ci_probs(0.99)

```

```{r}
quantile(bootstrap_means$mean_year, ci_probs(0.90) ) 

#This method is called the percentile based method.
```

##### Function Syntax 

```{r}
foo <- function(x) x^2
foo(3)

#This is an anonymous function and does not need to be assigned to a variable in order to call it (note: brackets are needed to call it):
{function(x) x^2}(5)

function(x) x^2 (5)

{. %>% raise_to_power(2)}(4)

\(x) x^2 ## new anonymous function syntax function replaced with backslash

bar <- \(x) x^2
bar(3)

{\(x) x^2}(6)

```

Anonymous functions are most useful as arguments to other functions.

### Using normal model to get interval

Here I use an anonymous pipeline as an argument to `geom_function`, which plots functions. We wish to plot a normal density curve, but with some arguments filled in with computed values from elsewhere.

```{r}
ggplot(bootstrap_means) + aes(x=mean_year) + 
  # Changed count to density on the y-axis
  geom_histogram(aes(y=..density..), center=0, binwidth=1) + 
  geom_function(fun=. %>% dnorm(mean=mean(bootstrap_means$mean_year), 
                                sd=sd(bootstrap_means$mean_year)), col="red") + 
  #Using only "dnorm" will give us the standard normal density curve, so we write a function using a pipe that will give us the curve that matches our data. The function slightly modifies dnorm by giving it a specific mean and standard deviation. 
  xlim(1985, 2005)
```

Instead of using the histogram to find our quantiles we can use the normal density curve. The `qnorm` function will do the theory-based version of what `quantile` does for a data set.

```{r}
ci_probs(0.95) %>% qnorm(mean=mean(bootstrap_means$mean_year), 
                                sd=sd(bootstrap_means$mean_year))
#This code looks up the quantiles from the "qnorm" data (qnorm for Quantile normal) given a specific mean and standard deviation. In this case, using the bootstrap_means data. 
ci_probs(0.95) %>% quantile(bootstrap_means$mean_year, .)
```


By default the pipe function inserts piped data to the first argument in the piped function. By using magrittr we can tell the pipe to place the data in a specific location by using a "." 

```{r}
level = 0.95
mean_hist + geom_vline(xintercept = ci_probs(level) %>% 
                         qnorm(mean=mean(bootstrap_means$mean_year), 
                               sd=sd(bootstrap_means$mean_year)), 
                       col="purple") + 
  geom_vline(xintercept = quantile(bootstrap_means$mean_year, ci_probs(level)), col="red")
```

This graph has the quantiles computed by the normal based method in purple, and the percentile based method in red. Even though they are not the exact same values, they are effectively the same.

You can only use the normal based method if your data fits a standard bell curve distribution, in cases where your data do not fit a bell curve, you can use the percentile based method. (Didn't mean to caps lock oops. Sorry!)

### Theory-based interval using `t.test`


Using a t-test and giving it the original data, you can compute a 95% confidence interval. This can be changed by changing the "level" parameter in the t.test() function.

```{r}
t.test(pennies_sample$year, level=0.95)
```

The previous approach isn't very "tidy", by using the with() function we can call specific columns in a dataframe without using `$` repeatedly. 

```{r}
with(pennies_sample, t.test(year)) #This code accomplishes the same thing as the previous t.test() function in a "tidy"-er manner. In this case we refer to the columns directly by name instead of using the "$" operator.
```

Using the "magrittr" pipe library, instead of using "%>%", we use the exposition "%$%". It tells the function following the exposition to look in the left side for data. 

```{r}
pennies %$% t.test(year) #This code accomplishes the same thing as the previous two t-tests in a more pipeline-style syntax. 

bootstrap_means %$% qnorm(ci_probs(0.95), mean=mean(mean_year), sd=sd(mean_year)) #This looks up the normal confidence interval values using the magrittr exposition syntax instead of the "$" operator. 
```

## The `mythbusters_yawn` question


Did people exposed to the "yawn" treatment yawn more or less than the people exposed to the control group of people.

16 people in the control group  
34 people in the experimental group  
*The sample sizes are not even*  

We will look at this question using _proportions_. In the control group, 25% of people in the group yawned, while in the experimental group, 29.4% of people yawned. We wanna answer the question of whether or not the difference of 4.4% is big. We will use Bootstrapping to answer this question. 

```{r}
mythbusters_yawn %>% count(group,yawn)
mythbusters_yawn %>% count(group,yawn) %>% 
  group_by(group) %>% # Group the data by group, this doesn't change the data.
  mutate(prop=proportions(n)) # Compute the proportions and add them to a column "prop". 
```

Alternatively, group the data by the "group", then create a summary table computing the mean number of people which responded with a yawn in each group. This process is done by changing the response variable into a binary response either True or False (1 or 0), then adding up the binary responses and dividing by the total number of observations. 

```{r}
mythbusters_yawn %>% group_by(group) %>% summarize(p_yawn = mean(yawn=="yes"))

# Intermediate step to understand why the above works
mythbusters_yawn %>% mutate(yawn_yes = (yawn=="yes"))


mythbusters_yawn %>% group_by(group) %>% summarize(p_yawn = mean(yawn=="yes")) %>% summarize(p_diff = diff(p_yawn)) # Diff is a function which computes the differences between one element and the next. 

```

Based on the output we can see that there is a difference of 4.4% between the two groups.

Use bootstrapping to investigate the variability of the `p_diff` statistic.


```{r}
mythbusters_yawn %>% slice_sample(prop=1, replace=TRUE) %>% group_by(group) %>% summarize(p_yawn = mean(yawn=="yes")) %>% summarize(p_diff = diff(p_yawn))
```

Looking at the output of the previous chunk after running it multiple times, the `p_diff` statistic jumps around, which makes it clear that we are not able to accurately predict the output based on the small sample size of the dataset.

Rerunning the previous code 

```{r}
diff_p_stat <- . %>% group_by(group) %>% summarize(p_yawn = mean(yawn=="yes")) %>% summarize(p_diff = diff(p_yawn)) # This function calculates the p_diff statistic.
```

The function can be applied to either a singular dataset, or a bootstrapped sample.

```{r}
diff_p_stat(mythbusters_yawn) # Single dataset
mythbusters_yawn %>% diff_p_stat # Same as previous line of code, except using pipe.

mythbusters_yawn %>% slice_sample(prop=1, replace=TRUE) %>% diff_p_stat # Bootstrapped sample.

boot_p_diffs <- rerun(2000, { 
  mythbusters_yawn %>% slice_sample(prop=1, replace=TRUE) %>% diff_p_stat
  }) %>% bind_rows

glimpse(boot_p_diffs)
```

Inspection of our results using a density plot.

```{r}
ggplot(boot_p_diffs) + aes(x=p_diff) + geom_density() + 
  geom_vline(xintercept=0.044, col="red")
```

Looking at the plot, we can see our estimate of the difference of 4.4% is close to zero in the grand scheme of things. Our estimate can be off by **up to 25%**, which is quite large on the scale of the numbers we are talking about in this instance. 


### Within group bootstrapping

```{r}
mythbusters_yawn %>% count(group,yawn) %>% group_by(group) %>% summarize(sum(n))

mythbusters_yawn %>% slice_sample(prop=1, replace=TRUE) %>% count(group,yawn) %>% group_by(group) %>% summarize(sum(n))
```

By treating the sample sizes as being fixed, we can hope to explain a bit more of the variability we see in the proportion. In our basic resampling scheme, we can end up with different numbers of people in the treatment and control groups. The sum of the two groups will always end up being 50 however. In order to keep the sample sizes fixed throughout the resampling process; we can add an extra `group_by(group)` to the pipe to make the `slice_sample()` work within the grouping. The count of yawn responses will change from sample to sample, however the total number of people in each group will remain the same. 


```{r}
mythbusters_yawn %>% 
  group_by(group) %>% # This extra group_by() function will make the resampling process adhere to the set number of people in each group (In this case, 16 in control group and 32 in the experimental group) 
  slice_sample(prop=1, replace=TRUE) %>% count(group,yawn) %>% group_by(group) %>% summarize(sum(n))

mythbusters_yawn %>% group_by(group) %>% slice_sample(prop=1, replace=TRUE) %>% count(group,yawn)

```

```{r}
boot_group_p_diffs <- rerun(2000, { 
  mythbusters_yawn %>% group_by(group) %>% slice_sample(prop=1, replace=TRUE) %>% diff_p_stat
  }) %>% bind_rows

ggplot(boot_group_p_diffs) + aes(x=p_diff) + geom_density()
```

Changing the resampling process to use fixed group sample sizes did not change the overall variability significantly, however it did eliminate the possibility of variability being introduced due to different numbers of people in each group across different resamples. Our CIs remain decently wide. 

```{r}
boot_p_diffs %>% summarize(p=ci_probs(0.95), bounds=quantile(p_diff, p))

boot_group_p_diffs %>% summarize(p=ci_probs(0.95), bounds=quantile(p_diff, p))

```



## Exercises

1. Data set `openintro::solar`. Filter the data to `location == "Haight_Ashbury"` only and use the filtered data in the rest of these problems. Make a histogram or density plot of the `kwh` power production variable.

2. Give the average power production in the data set, and then use bootstrapping to provide a 95% confidence interval for mean daily power production of the solar array. Produce two intervals, using the percentile and Normal based methods. 

3. In power production, consistency can be as important of a consideration as average output. Estimate the standard deviation (`sd`) for the power production. Then use bootstrapping to provide a 95% confidence interval for the standard deviation of the array's daily power production.

--------------

Think carefully about how you want to structure the data before doing the bootstrap resampling in the two-sample questions below. We want to simulate a new sample of separate kicks of both types of ball. Simply resampling rows from the data set doesn't quite capture this using the data in the shape provided. 

4. Investigate the `openintro::helium` data, make a data visualization comparing the two types of footballs. (boxplots, violin plots, overlapping density plots are all possibilities)

5. Provide a point estimate of the difference in the means between between the air and helium filled footballs. Then use bootstrapping to provide a 95% confidence interval estimate of the difference in the average distance traveled the two types of ball. You may use either a percentile or normal based method for obtaining the interval.

6. Suppose instead of comparing the mean distance traveled, we were interested in comparing the amount of variability in the distance. A _variance ratio_ is one way of comparing the variability of two groups. Compute the variance (`var`) of one ball type and divide it by the variance of the other type. Use bootstrapping to provide a 95% confidence interval for the value of the variance ratio.


##### Stats side note

Robustness: how much does an individual data point impact a summary statistic.

The min and maximum values of a data set are not very robust data points. They can change an arbitrarily large amount if a single data point becomes very large/small.

The median is a very robust statistic, in order to change the median significantly you must alter half of the data values.


